{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于BiLSTM + Attention 的中文分词\n",
    "20307140044 李培基"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import collections\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取词嵌入矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词嵌入表总数： 11327\n",
      "Embedding Matrix Shape:  torch.Size([11327, 50])\n"
     ]
    }
   ],
   "source": [
    "def read_embedding(file_path = \"./gigaword_chn.all.a2b.uni.ite50.vec\"):\n",
    "    data = {} # 单词和向量的映射字典\n",
    "    tokens = [] # 单词的列表\n",
    "    embedding = [] # 词嵌入矩阵\n",
    "    with open(file_path,\"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            line = line.strip().split(' ')\n",
    "            if len(line) < 2:\n",
    "                line = f.readline()\n",
    "                continue\n",
    "            word = line[0]\n",
    "            tokens.append(word)\n",
    "            vec = [float(item) for item in line[1:]]\n",
    "            embedding.append(vec)\n",
    "            # round(float(item), 3)\n",
    "            data[word] = vec\n",
    "            line = f.readline()\n",
    "    return tokens, torch.tensor(embedding)\n",
    "\n",
    "\n",
    "\n",
    "tokens, embedding = read_embedding(\"./gigaword_chn.all.a2b.uni.ite50.vec\")\n",
    "print('词嵌入表总数：',len(tokens))\n",
    "print('Embedding Matrix Shape: ', embedding.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立Vocab类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab 长度: 11329\n",
      "embedding shape: torch.Size([11329, 50])\n"
     ]
    }
   ],
   "source": [
    "class Vocab:\n",
    "    '''\n",
    "        从词嵌入矩阵建立词表Vocab\n",
    "        有些其他做法则是从训练数据建立词表,\n",
    "    '''\n",
    "    def __init__(self, tokens, embedding, reserved_tokens=['<pad>','<unk>']):\n",
    "        self.idx_to_token = reserved_tokens\n",
    "        self.token_to_idx = {token: idx\n",
    "                                    for idx, token in enumerate(self.idx_to_token)}\n",
    "        for token in tokens:\n",
    "            assert token not in self.token_to_idx # 确保一一映射\n",
    "            self.idx_to_token.append(token) \n",
    "            self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "        unk_vec = torch.mean(embedding,dim = 0) # 计算未知词元的向量，可以使用词向量的平均值\n",
    "        pad_vec = torch.zeros(embedding.size(1)) # 创建一个全零的向量，作为填充词元的向量\n",
    "        self.embedding = torch.cat((pad_vec.unsqueeze(0), unk_vec.unsqueeze(0), embedding), dim=0) # 将填充和未知词元的向量添加到词向量张量的开头\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    @property\n",
    "    def pad(self):  \n",
    "        return 0\n",
    "     \n",
    "    @property\n",
    "    def unk(self):  \n",
    "        return 1\n",
    "    \n",
    "    def __getitem__(self, tokens):\n",
    "        '''\n",
    "        从word到序号:\n",
    "        \n",
    "        输入token:可以是单个词或者元组列表, 均可以实现返回\n",
    "        \n",
    "        存在一次非常巧妙的递归调用\n",
    "        '''\n",
    "        if not isinstance(tokens, (list, tuple)): # 如果tokens是一个单词, 单独返回单词\n",
    "            return self.token_to_idx.get(tokens, self.unk) # 若没有出现在词表中，会返回unk\n",
    "        return [self.__getitem__(token) for token in tokens] # tokens是一串单词，以列表形式返回，即使这串单词中有未出现在词表中的单词也没有关系\n",
    "\n",
    "    \n",
    "    def to_tokens(self, indices):\n",
    "        '''\n",
    "        从序号到word\n",
    "        '''\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "    \n",
    "vocab = Vocab(tokens, embedding)\n",
    "print('vocab 长度:',len(vocab))\n",
    "print('embedding shape:', vocab.embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据集的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['迈', '向', '充', '满', '希', '望', '的', '新', '世', '纪', '—', '—', '一', '九', '九', '八', '年', '新', '年', '讲', '话', '（', '附', '图', '片', '１', '张', '）']\n",
      "['B-CWS', 'E-CWS', 'B-CWS', 'E-CWS', 'B-CWS', 'E-CWS', 'S-CWS', 'S-CWS', 'B-CWS', 'E-CWS', 'B-CWS', 'E-CWS', 'B-CWS', 'I-CWS', 'I-CWS', 'I-CWS', 'E-CWS', 'B-CWS', 'E-CWS', 'B-CWS', 'E-CWS', 'S-CWS', 'S-CWS', 'B-CWS', 'E-CWS', 'S-CWS', 'S-CWS', 'S-CWS']\n",
      "[2182, 238, 913, 1039, 395, 368, 4, 33, 252, 853, 1298, 1298, 8, 147, 147, 180, 18, 33, 18, 1207, 706, 1, 1174, 974, 787, 1, 617, 1]\n",
      "['迈', '向', '充', '满', '希', '望', '的', '新', '世', '纪', '—', '—', '一', '九', '九', '八', '年', '新', '年', '讲', '话', '<unk>', '附', '图', '片', '<unk>', '张', '<unk>']\n",
      "[0, 1, 0, 1, 0, 1, 2, 2, 0, 1, 0, 1, 0, 3, 3, 3, 1, 0, 1, 0, 1, 2, 2, 0, 1, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "label_to_idx = {'B-CWS': 0, 'E-CWS': 1, 'S-CWS': 2, 'I-CWS': 3}\n",
    "idx_to_label = ['B-CWS', 'E-CWS', 'S-CWS', 'I-CWS']\n",
    "\n",
    "def read_data(filename):\n",
    "    sentences = [] # 存储句子列表\n",
    "    labels = [] # 存储标签列表\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        sentence = [] # 存储当前句子\n",
    "        label = [] # 存储当前标签\n",
    "        for line in f:\n",
    "            line = line.strip() # 去掉首尾空格\n",
    "            if line: # 如果不是空行\n",
    "                char, tag = line.split() # 分割字符和标签\n",
    "                sentence.append(char) # 添加到当前句子\n",
    "                label.append(label_to_idx[tag]) # 添加到当前标签\n",
    "            else: # 如果是空行\n",
    "                if sentence: # 如果当前句子不为空\n",
    "                    sentences.append(sentence) # 添加到句子列表\n",
    "                    labels.append(label) # 添加到标签列表\n",
    "                    sentence = [] # 清空当前句子\n",
    "                    label = [] # 清空当前标签\n",
    "    return sentences, labels\n",
    "\n",
    "sentences, labels = read_data('./pku/train.txt')\n",
    "print(sentences[0])\n",
    "print([idx_to_label[i] for i in labels[0]])\n",
    "print(vocab[sentences[0]])\n",
    "print(vocab.to_tokens(vocab[sentences[0]]))\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44565 4590\n",
      "(tensor([2182,  238,  913, 1039,  395,  368,    4,   33,  252,  853, 1298, 1298,\n",
      "           8,  147,  147,  180,   18,   33,   18, 1207,  706,    1, 1174,  974,\n",
      "         787,    1,  617,    1]), tensor([0, 1, 0, 1, 0, 1, 2, 2, 0, 1, 0, 1, 0, 3, 3, 3, 1, 0, 1, 0, 1, 2, 2, 0,\n",
      "        1, 2, 2, 2]))\n"
     ]
    }
   ],
   "source": [
    "class CWSDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, vocab):\n",
    "        assert len(sentences) == len(labels) # 确保句子和标签的数量相同\n",
    "        self.sentences = sentences # 存储句子列表\n",
    "        self.labels = labels # 存储标签列表\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences) # 返回数据集的大小\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.vocab[self.sentences[idx]]), torch.tensor(self.labels[idx]) # 返回指定索引的句子和标签\n",
    "\n",
    "# 读取数据文件，得到句子和标签列表\n",
    "train_sentences, train_labels = read_data('./pku/train.txt')\n",
    "test_sentences, test_labels = read_data('./pku/test.txt')\n",
    "\n",
    "# 创建数据集对象，传入句子和标签列表\n",
    "train_dataset = CWSDataset(train_sentences, train_labels, vocab)\n",
    "test_dataset = CWSDataset(test_sentences, test_labels, vocab)\n",
    "print(len(train_dataset),len(test_dataset))\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 85])\n",
      "torch.Size([32, 85])\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(data):\n",
    "    sentences, labels = zip(*data) # 分离句子和标签序列    \n",
    "    # 获取每个句子的长度，并按照长度降序排列\n",
    "    lengths = [s.size(0) for s in sentences]\n",
    "    lengths, indices = torch.sort(torch.tensor(lengths), descending=True)\n",
    "    sentences = [sentences[i] for i in indices]\n",
    "    labels = [labels[i] for i in indices]\n",
    "    # 对不同长度的句子和标签进行填充，指定填充值为0，并且按照长度降序排列\n",
    "    sentences = torch.nn.utils.rnn.pad_sequence(sentences, padding_value=0, batch_first=True)\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(labels, padding_value=4, batch_first=True)\n",
    "    \n",
    "    return sentences, labels, lengths\n",
    "\n",
    "batch_size = 32\n",
    "# 创建数据加载器对象，传入数据集对象，批次大小，是否打乱顺序，以及collate_fn函数\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# 测试\n",
    "for sentences, labels, lengths in train_loader: # 遍历每个批次的数据\n",
    "    print(sentences.shape) # 打印句子的张量\n",
    "    print(labels.shape) # 打印标签的张量\n",
    "    break# 可以在打印一个批次后就退出循环，或者继续打印更多的批次\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_key_padding_mask(lengths):\n",
    "    '''\n",
    "        input: lengths 是有效长度的列表 (batch_size)\n",
    "        output: 一个key_padding_mask: 用于multihead_attn\n",
    "        由gpt3.5辅助编写\n",
    "    '''\n",
    "    batch_size = len(lengths)\n",
    "    max_seq_len = max(lengths)\n",
    "    # 创建一个形状为 (batch_size, max_seq_len) 的零矩阵\n",
    "    key_padding_mask = torch.zeros(batch_size, max_seq_len, dtype=torch.bool).to(device)\n",
    "\n",
    "    for i, length in enumerate(lengths):\n",
    "        # 对于每个样本，将有效长度之后的位置设置为True\n",
    "        key_padding_mask[i, length:] = True\n",
    "\n",
    "    return key_padding_mask\n",
    "\n",
    "class RNNModel(torch.nn.Module):\n",
    "    def __init__(self, embedding_matrix, embed_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNNModel, self).__init__()\n",
    "        # 创建一个预训练的embedding层\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(embedding_matrix)\n",
    "        # 创建一个双向的LSTM层\n",
    "        self.lstm = torch.nn.LSTM(input_size=embed_size, hidden_size=hidden_size, num_layers=num_layers, bidirectional=True)\n",
    "        # 创建一个全连接层\n",
    "        self.linear = torch.nn.Linear(hidden_size * 2, num_classes)\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "        # x: 可以认为是 (batch_size, seq_len) \n",
    "        x = self.embedding(x) # x: (batch_size, seq_len, embed_size)\n",
    "        # 使用pack_padded_sequence将填充后的序列打包成PackedSequence对象\n",
    "        x = torch.nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True)\n",
    "        x, _ = self.lstm(x) \n",
    "        x, out_len = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
    "        # 解包: (batch_size, seq_len, hidden_size * 2)\n",
    "        # 将LSTM的输出序列传入全连接层，得到最终的输出序列\n",
    "        output = self.linear(x) # x: (batch_size, seq_len, num_classes)\n",
    "        return output\n",
    "\n",
    "class RNNAttention(torch.nn.Module):\n",
    "    def __init__(self, embedding_matrix, embed_size, hidden_size, num_layers, num_classes, attn_heads, dropout):\n",
    "        super(RNNAttention, self).__init__()\n",
    "        # 创建一个预训练的embedding层\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(embedding_matrix)\n",
    "        # 创建一个双向的LSTM层\n",
    "        self.lstm = torch.nn.LSTM(input_size=embed_size, hidden_size=hidden_size, num_layers=num_layers, bidirectional=True)\n",
    "        # 创建一个全连接层\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=hidden_size*2, num_heads=attn_heads, dropout=dropout)\n",
    "        self.linear = torch.nn.Linear(hidden_size * 2, num_classes)\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "        # x: 可以认为是 (batch_size, seq_len) \n",
    "        x = self.embedding(x) # x: (batch_size, seq_len, embed_size)\n",
    "        # 使用pack_padded_sequence将填充后的序列打包成PackedSequence对象\n",
    "        x = torch.nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True)\n",
    "        x, _ = self.lstm(x) \n",
    "        x, out_len = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
    "        # 解包: (batch_size, seq_len, hidden_size * 2)\n",
    "        # 将LSTM的输出序列传入全连接层，得到最终的输出序列\n",
    "        key_padding_mask = create_key_padding_mask(lengths)\n",
    "        x = x.permute(1,0,2) # multiheadattention 要求第一维度是seq_len\n",
    "        x,_ = self.multihead_attn(x,x,x,key_padding_mask)\n",
    "        x = x.permute(1,0,2)\n",
    "        output = self.linear(x) # x: (batch_size, seq_len, num_classes)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 选择模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN\n",
    "model = RNNModel(embedding_matrix=vocab.embedding, embed_size=50,hidden_size=128,num_layers=2,num_classes=4)\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=4) # 这个很必要 完全忽略padding的标签(对应index为4)\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN with Attention\n",
    "model = RNNAttention(embedding_matrix=vocab.embedding, embed_size=50,hidden_size=128,num_layers=2,num_classes=4,\n",
    "                     attn_heads=4, dropout=0.2)\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=4) # 这个很必要 完全忽略padding的标签(对应index为4)\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练与评测\n",
    "* 尝试不同超参数搜索组合，调整batch_size,dropout,lr等等多种方式\n",
    "* 测试集上micro-F1可达93.2%，macro-F1 可达 90.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate(predictions, labels):\n",
    "    '''\n",
    "        计算micro-f1 和 macro-f1\n",
    "        需要注意的是, 这个任务中每个词只属于一个类别, 所以micro-f1等价于acc\n",
    "    '''\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    TP = [0,0,0,0]\n",
    "    FP = [0,0,0,0]\n",
    "    FN = [0,0,0,0]\n",
    "    P = [0,0,0,0]\n",
    "    R = [0,0,0,0]\n",
    "    for i in range(predictions.shape[0]):\n",
    "        for j in range(predictions.shape[1]):\n",
    "            if labels[i][j] == 4:\n",
    "                continue\n",
    "            if predictions[i][j] == labels[i][j]:\n",
    "                correct += 1\n",
    "                TP[predictions[i][j]] += 1\n",
    "            else:\n",
    "                FN[predictions[i][j]] += 1\n",
    "                FP[labels[i][j]] += 1\n",
    "            total += 1\n",
    "    \n",
    "    accuracy = correct / total # 即micro-f1\n",
    "    for i in range(4):\n",
    "        P[i] = TP[i]/(TP[i] + FP[i] + 1e-9)\n",
    "        R[i] = TP[i]/(TP[i] + FN[i] + 1e-9)\n",
    "    macro_f1 = sum([ 2*P[i]*R[i]/(P[i] + R[i] + 1e-9) for i in range(4)])/4\n",
    "    return accuracy, macro_f1\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, num_epochs, loss_fn, log_freq = 1000, save_path = 'model.pth', policy = 'micro'):\n",
    "    # device = 'cpu'\n",
    "    print(f'Start Training, Dev Policy:{policy}_f1 ')\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    best_micro = 0.0\n",
    "    best_macro = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        model.train()\n",
    "        # 遍历数据加载器对象，得到每个批次的数据\n",
    "        for batch_idx, (sentences, labels, lengths) in enumerate(train_loader):\n",
    "            sentences = sentences.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(sentences, lengths) # outputs: (batch_size, seq_len, num_classes)\n",
    "            preds = torch.argmax(outputs,dim=2) # (batch_size, seq_len)\n",
    "            if(batch_idx%log_freq == 0):\n",
    "                micro, macro = calculate(preds,labels)\n",
    "                print(f'Log: current micro_f1(acc):{micro}, current macro_f1:{macro}')\n",
    "            outputs = outputs.permute(0,2,1) # CrossEntropy 要求输出的第二维度是分类数\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch + 1}, average loss: {avg_loss:.4f}')\n",
    "        # 评测\n",
    "        model.eval()\n",
    "        test_micro = 0.0\n",
    "        test_macro = 0.0\n",
    "        for batch_idx, (sentences, labels, lengths) in enumerate(test_loader):\n",
    "            sentences = sentences.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(sentences, lengths) # outputs: (batch_size, seq_len, num_classes)\n",
    "            preds = torch.argmax(outputs,dim=2) # (batch_size, seq_len)\n",
    "            micro,macro = calculate(preds,labels)\n",
    "            test_micro += micro\n",
    "            test_macro += macro\n",
    "        test_micro = test_micro/len(test_loader)\n",
    "        test_macro = test_macro/len(test_loader)\n",
    "        print(f'Test: micro_f1(acc) on test set:{test_micro}, macro_f1 on test set: {test_macro}')\n",
    "        if(test_micro > best_micro):\n",
    "            best_micro = test_micro\n",
    "            if policy != 'macro':\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                print(f'Best Model Parameter updated and saved')\n",
    "        if(test_macro > best_macro):\n",
    "            best_macro = test_macro\n",
    "            if policy == 'macro':\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                print(f'Best Model Parameter updated and saved')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training, Dev Policy:micro_f1 \n",
      "Log: current micro_f1(acc):0.22630718954248366, current macro_f1:0.13551542099990785\n",
      "Log: current micro_f1(acc):0.8971036585365854, current macro_f1:0.8715305059853139\n",
      "Epoch 1, average loss: 0.3338\n",
      "Test: micro_f1(acc) on test set:0.915131676351508, macro_f1 on test set: 0.8821977666302673\n",
      "Best Model Parameter updated and saved\n",
      "Log: current micro_f1(acc):0.9327102803738317, current macro_f1:0.9110797069663046\n",
      "Log: current micro_f1(acc):0.9695885509838998, current macro_f1:0.9635713119248549\n",
      "Epoch 2, average loss: 0.1570\n",
      "Test: micro_f1(acc) on test set:0.9256190366373019, macro_f1 on test set: 0.8959623733585168\n",
      "Best Model Parameter updated and saved\n",
      "Log: current micro_f1(acc):0.9570647931303669, current macro_f1:0.9435405855257601\n",
      "Log: current micro_f1(acc):0.9584996009577015, current macro_f1:0.9521265291079267\n",
      "Epoch 3, average loss: 0.1223\n",
      "Test: micro_f1(acc) on test set:0.9296235784946032, macro_f1 on test set: 0.9032053904347437\n",
      "Best Model Parameter updated and saved\n",
      "Log: current micro_f1(acc):0.9682675814751286, current macro_f1:0.9544801154130816\n",
      "Log: current micro_f1(acc):0.9658813972380179, current macro_f1:0.9546245972636607\n",
      "Epoch 4, average loss: 0.1006\n",
      "Test: micro_f1(acc) on test set:0.9278169480638934, macro_f1 on test set: 0.8988165811377059\n",
      "Log: current micro_f1(acc):0.9751937984496124, current macro_f1:0.9680697052084549\n",
      "Log: current micro_f1(acc):0.9801375095492743, current macro_f1:0.9763252530975368\n",
      "Epoch 5, average loss: 0.0847\n",
      "Test: micro_f1(acc) on test set:0.9282624476311081, macro_f1 on test set: 0.9009456249731089\n",
      "Log: current micro_f1(acc):0.9762589928057553, current macro_f1:0.9714196780235869\n",
      "Log: current micro_f1(acc):0.9769008662175168, current macro_f1:0.9582662711389487\n",
      "Epoch 6, average loss: 0.0723\n",
      "Test: micro_f1(acc) on test set:0.929724247951504, macro_f1 on test set: 0.9030121096822469\n",
      "Best Model Parameter updated and saved\n",
      "Log: current micro_f1(acc):0.9884809215262779, current macro_f1:0.9851029896404161\n",
      "Log: current micro_f1(acc):0.9785247432306255, current macro_f1:0.97510493391278\n",
      "Epoch 7, average loss: 0.0629\n",
      "Test: micro_f1(acc) on test set:0.9324583516767819, macro_f1 on test set: 0.9078689787517583\n",
      "Best Model Parameter updated and saved\n",
      "Log: current micro_f1(acc):0.9767441860465116, current macro_f1:0.9716461227960289\n",
      "Log: current micro_f1(acc):0.9907142857142858, current macro_f1:0.9865819676147116\n",
      "Epoch 8, average loss: 0.0548\n",
      "Test: micro_f1(acc) on test set:0.9303931004985361, macro_f1 on test set: 0.9043471432083008\n",
      "Log: current micro_f1(acc):0.9859030837004406, current macro_f1:0.9851717530867851\n",
      "Log: current micro_f1(acc):0.9832669322709163, current macro_f1:0.9715591690873839\n",
      "Epoch 9, average loss: 0.0486\n",
      "Test: micro_f1(acc) on test set:0.9324557081041884, macro_f1 on test set: 0.9068079001115928\n",
      "Log: current micro_f1(acc):0.9914651493598862, current macro_f1:0.9885888815767808\n",
      "Log: current micro_f1(acc):0.9925249169435216, current macro_f1:0.9927030073600468\n",
      "Epoch 10, average loss: 0.0432\n",
      "Test: micro_f1(acc) on test set:0.9313028988232535, macro_f1 on test set: 0.9052643356460099\n",
      "Log: current micro_f1(acc):0.9868725868725868, current macro_f1:0.9837046950890773\n",
      "Log: current micro_f1(acc):0.9767441860465116, current macro_f1:0.9691077741401873\n",
      "Epoch 11, average loss: 0.0387\n",
      "Test: micro_f1(acc) on test set:0.9306258213368117, macro_f1 on test set: 0.9053464711455461\n",
      "Log: current micro_f1(acc):0.9920704845814978, current macro_f1:0.9859181622624806\n",
      "Log: current micro_f1(acc):0.986088379705401, current macro_f1:0.9833796793136175\n",
      "Epoch 12, average loss: 0.0358\n",
      "Test: micro_f1(acc) on test set:0.9298375482721992, macro_f1 on test set: 0.9039096654715277\n",
      "Log: current micro_f1(acc):0.9877488514548239, current macro_f1:0.9868293747335256\n",
      "Log: current micro_f1(acc):0.9936363636363637, current macro_f1:0.9924552743982478\n",
      "Epoch 13, average loss: 0.0328\n",
      "Test: micro_f1(acc) on test set:0.9308705382099364, macro_f1 on test set: 0.90443922149216\n",
      "Log: current micro_f1(acc):0.9916349809885932, current macro_f1:0.9890425512123838\n",
      "Log: current micro_f1(acc):0.9839181286549707, current macro_f1:0.9803303801172255\n",
      "Epoch 14, average loss: 0.0308\n",
      "Test: micro_f1(acc) on test set:0.9317380856659407, macro_f1 on test set: 0.9079359326884073\n",
      "Log: current micro_f1(acc):0.9924981245311327, current macro_f1:0.9918230374879047\n",
      "Log: current micro_f1(acc):0.9917763157894737, current macro_f1:0.9878788991497528\n",
      "Epoch 15, average loss: 0.0285\n",
      "Test: micro_f1(acc) on test set:0.9323419687818943, macro_f1 on test set: 0.9073064660760801\n",
      "Log: current micro_f1(acc):0.9923430321592649, current macro_f1:0.9909064435045395\n",
      "Log: current micro_f1(acc):0.9902234636871509, current macro_f1:0.9892746437025912\n",
      "Epoch 16, average loss: 0.0272\n",
      "Test: micro_f1(acc) on test set:0.9310432856075276, macro_f1 on test set: 0.904723939292683\n",
      "Log: current micro_f1(acc):0.9960681520314548, current macro_f1:0.9939748078437339\n",
      "Log: current micro_f1(acc):0.992671009771987, current macro_f1:0.9902335071116489\n",
      "Epoch 17, average loss: 0.0257\n",
      "Test: micro_f1(acc) on test set:0.9296095387721729, macro_f1 on test set: 0.9031410085085788\n",
      "Log: current micro_f1(acc):0.9941860465116279, current macro_f1:0.9929381066941976\n",
      "Log: current micro_f1(acc):0.993025283347864, current macro_f1:0.9904049266309736\n",
      "Epoch 18, average loss: 0.0245\n",
      "Test: micro_f1(acc) on test set:0.9301972330154059, macro_f1 on test set: 0.9045700112771012\n",
      "Log: current micro_f1(acc):0.9922480620155039, current macro_f1:0.9875499016014637\n",
      "Log: current micro_f1(acc):0.9918772563176895, current macro_f1:0.9896020731084572\n",
      "Epoch 19, average loss: 0.0234\n",
      "Test: micro_f1(acc) on test set:0.9269764713210956, macro_f1 on test set: 0.9006267163338137\n",
      "Log: current micro_f1(acc):0.9936619718309859, current macro_f1:0.9905831804999152\n",
      "Log: current micro_f1(acc):0.9975713418336369, current macro_f1:0.9955496019556914\n",
      "Epoch 20, average loss: 0.0226\n",
      "Test: micro_f1(acc) on test set:0.9307306242471717, macro_f1 on test set: 0.9043377409392874\n"
     ]
    }
   ],
   "source": [
    "train(model,train_loader,optimizer,num_epochs,loss_fn, log_freq=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在这里输入要进行分词的中文语句\n",
    "input = '感谢复旦大学自然语言处理实验室的黄老师和几位助教的指导，我在本次实验中收获了很多'\n",
    "init_string = input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_split(input, model_path = './model.pth'):\n",
    "    input = [char for char in input]\n",
    "    model = RNNAttention(embedding_matrix=vocab.embedding, embed_size=50,hidden_size=128,num_layers=2,num_classes=4,\n",
    "                        attn_heads=4, dropout=0.2)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    x = torch.tensor(vocab[input]).unsqueeze(0).to(device)\n",
    "    output = model(x,[x.shape[1]])\n",
    "    preds = torch.argmax(output,dim=2).squeeze().tolist()\n",
    "    assert len(input) == len(preds)\n",
    "    res = []\n",
    "    char = ''\n",
    "    for idx in range(len(input)):\n",
    "        if preds[idx] == 0: # B\n",
    "            char += input[idx]\n",
    "            continue\n",
    "        if preds[idx] == 1:\n",
    "            char += input[idx]\n",
    "            res.append(char)\n",
    "            char = ''\n",
    "            continue\n",
    "        if preds[idx] == 2:\n",
    "            if char!='': # 自己加一点约束\n",
    "                res.append(char)\n",
    "            res.append(input[idx])\n",
    "        else:\n",
    "            char += input[idx]\n",
    "            continue\n",
    "            \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "感谢复旦大学自然语言处理实验室的黄老师和几位助教的指导，我在本次实验中收获了很多\n",
      "['感谢', '复旦', '大学', '自然', '语言', '处理', '实验室', '的', '黄', '老师', '和', '几', '位', '助教', '的', '指导', '，', '我', '在', '本次', '实验', '中', '收获', '了', '很多']\n"
     ]
    }
   ],
   "source": [
    "res = word_split(input)\n",
    "print(input)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结与不足\n",
    "* 把分词任务从一个机器学习的统计任务变为了深度学习的分类标记任务：深度学习的本质也是统计\n",
    "* 不同的类别之间应该有转移概率的约束，其实应用上条件随机场(CRF)会更好，但是我的服务器断网了，有些包没法安装，以后再试试。\n",
    "* 比较令人欣喜的是，从结果上来看，模型自己学到了这些约束\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 清除显存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import os\n",
    "pid = os.getpid()\n",
    "!kill -9 $pid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlptorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
